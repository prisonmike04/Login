{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prisonmike04/Login/blob/main/spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb7PL1CsWYT7"
      },
      "source": [
        "my spark project\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irAvG9zAU-Y5",
        "outputId": "6c4694ee-b3af-4a1b-ca1f-98eb5fb33fce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=26b66cdf1fa070c17b760db0958ba1c188c1195a8a853c5dd2437e20a63f4415\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "sc = SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "oCjLzKKRZuDr",
        "outputId": "eda805d1-66e3-4ac4-c8b8-93eb921c929d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.context.SparkContext</b><br/>def __init__(master: Optional[str]=None, appName: Optional[str]=None, sparkHome: Optional[str]=None, pyFiles: Optional[List[str]]=None, environment: Optional[Dict[str, Any]]=None, batchSize: int=0, serializer: &#x27;Serializer&#x27;=CPickleSerializer(), conf: Optional[SparkConf]=None, gateway: Optional[JavaGateway]=None, jsc: Optional[JavaObject]=None, profiler_cls: Type[BasicProfiler]=BasicProfiler, udf_profiler_cls: Type[UDFBasicProfiler]=UDFBasicProfiler, memory_profiler_cls: Type[MemoryProfiler]=MemoryProfiler)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/context.py</a>Main entry point for Spark functionality. A SparkContext represents the\n",
              "connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
              "broadcast variables on that cluster.\n",
              "\n",
              "When you create a new SparkContext, at least the master and app name should\n",
              "be set, either through the named parameters here or through `conf`.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "master : str, optional\n",
              "    Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
              "appName : str, optional\n",
              "    A name for your job, to display on the cluster web UI.\n",
              "sparkHome : str, optional\n",
              "    Location where Spark is installed on cluster nodes.\n",
              "pyFiles : list, optional\n",
              "    Collection of .zip or .py files to send to the cluster\n",
              "    and add to PYTHONPATH.  These can be paths on the local file\n",
              "    system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
              "environment : dict, optional\n",
              "    A dictionary of environment variables to set on\n",
              "    worker nodes.\n",
              "batchSize : int, optional, default 0\n",
              "    The number of Python objects represented as a single\n",
              "    Java object. Set 1 to disable batching, 0 to automatically choose\n",
              "    the batch size based on object sizes, or -1 to use an unlimited\n",
              "    batch size\n",
              "serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n",
              "    The serializer for RDDs.\n",
              "conf : :class:`SparkConf`, optional\n",
              "    An object setting Spark properties.\n",
              "gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n",
              "    Use an existing gateway and JVM, otherwise a new JVM\n",
              "    will be instantiated. This is only used internally.\n",
              "jsc : class:`py4j.java_gateway.JavaObject`, optional\n",
              "    The JavaSparkContext instance. This is only used internally.\n",
              "profiler_cls : type, optional, default :class:`BasicProfiler`\n",
              "    A class of custom Profiler used to do profiling\n",
              "udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n",
              "    A class of custom Profiler used to do udf profiling\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
              "the active :class:`SparkContext` before creating a new one.\n",
              "\n",
              ":class:`SparkContext` instance is not supported to share across multiple\n",
              "processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
              "Use threads instead for concurrent processing purpose.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; from pyspark.context import SparkContext\n",
              "&gt;&gt;&gt; sc = SparkContext(&#x27;local&#x27;, &#x27;test&#x27;)\n",
              "&gt;&gt;&gt; sc2 = SparkContext(&#x27;local&#x27;, &#x27;test2&#x27;) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
              "Traceback (most recent call last):\n",
              "    ...\n",
              "ValueError: ...</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 92);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "pyspark.context.SparkContext"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL-p6XIKa5u5",
        "outputId": "15bcc8f2-6905-4366-f300-2730d5c0927d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on SparkContext in module pyspark.context object:\n",
            "\n",
            "class SparkContext(builtins.object)\n",
            " |  SparkContext(master: Optional[str] = None, appName: Optional[str] = None, sparkHome: Optional[str] = None, pyFiles: Optional[List[str]] = None, environment: Optional[Dict[str, Any]] = None, batchSize: int = 0, serializer: 'Serializer' = CloudPickleSerializer(), conf: Optional[pyspark.conf.SparkConf] = None, gateway: Optional[py4j.java_gateway.JavaGateway] = None, jsc: Optional[py4j.java_gateway.JavaObject] = None, profiler_cls: Type[pyspark.profiler.BasicProfiler] = <class 'pyspark.profiler.BasicProfiler'>, udf_profiler_cls: Type[pyspark.profiler.UDFBasicProfiler] = <class 'pyspark.profiler.UDFBasicProfiler'>, memory_profiler_cls: Type[pyspark.profiler.MemoryProfiler] = <class 'pyspark.profiler.MemoryProfiler'>)\n",
            " |  \n",
            " |  Main entry point for Spark functionality. A SparkContext represents the\n",
            " |  connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
            " |  broadcast variables on that cluster.\n",
            " |  \n",
            " |  When you create a new SparkContext, at least the master and app name should\n",
            " |  be set, either through the named parameters here or through `conf`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  master : str, optional\n",
            " |      Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
            " |  appName : str, optional\n",
            " |      A name for your job, to display on the cluster web UI.\n",
            " |  sparkHome : str, optional\n",
            " |      Location where Spark is installed on cluster nodes.\n",
            " |  pyFiles : list, optional\n",
            " |      Collection of .zip or .py files to send to the cluster\n",
            " |      and add to PYTHONPATH.  These can be paths on the local file\n",
            " |      system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
            " |  environment : dict, optional\n",
            " |      A dictionary of environment variables to set on\n",
            " |      worker nodes.\n",
            " |  batchSize : int, optional, default 0\n",
            " |      The number of Python objects represented as a single\n",
            " |      Java object. Set 1 to disable batching, 0 to automatically choose\n",
            " |      the batch size based on object sizes, or -1 to use an unlimited\n",
            " |      batch size\n",
            " |  serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n",
            " |      The serializer for RDDs.\n",
            " |  conf : :class:`SparkConf`, optional\n",
            " |      An object setting Spark properties.\n",
            " |  gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n",
            " |      Use an existing gateway and JVM, otherwise a new JVM\n",
            " |      will be instantiated. This is only used internally.\n",
            " |  jsc : class:`py4j.java_gateway.JavaObject`, optional\n",
            " |      The JavaSparkContext instance. This is only used internally.\n",
            " |  profiler_cls : type, optional, default :class:`BasicProfiler`\n",
            " |      A class of custom Profiler used to do profiling\n",
            " |  udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n",
            " |      A class of custom Profiler used to do udf profiling\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
            " |  the active :class:`SparkContext` before creating a new one.\n",
            " |  \n",
            " |  :class:`SparkContext` instance is not supported to share across multiple\n",
            " |  processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
            " |  Use threads instead for concurrent processing purpose.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from pyspark.context import SparkContext\n",
            " |  >>> sc = SparkContext('local', 'test')\n",
            " |  >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            " |  Traceback (most recent call last):\n",
            " |      ...\n",
            " |  ValueError: ...\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __enter__(self) -> 'SparkContext'\n",
            " |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n",
            " |  \n",
            " |  __exit__(self, type: Optional[Type[BaseException]], value: Optional[BaseException], trace: Optional[traceback]) -> None\n",
            " |      Enable 'with SparkContext(...) as sc: app' syntax.\n",
            " |      \n",
            " |      Specifically stop the context on exit of the with block.\n",
            " |  \n",
            " |  __getnewargs__(self) -> NoReturn\n",
            " |  \n",
            " |  __init__(self, master: Optional[str] = None, appName: Optional[str] = None, sparkHome: Optional[str] = None, pyFiles: Optional[List[str]] = None, environment: Optional[Dict[str, Any]] = None, batchSize: int = 0, serializer: 'Serializer' = CloudPickleSerializer(), conf: Optional[pyspark.conf.SparkConf] = None, gateway: Optional[py4j.java_gateway.JavaGateway] = None, jsc: Optional[py4j.java_gateway.JavaObject] = None, profiler_cls: Type[pyspark.profiler.BasicProfiler] = <class 'pyspark.profiler.BasicProfiler'>, udf_profiler_cls: Type[pyspark.profiler.UDFBasicProfiler] = <class 'pyspark.profiler.UDFBasicProfiler'>, memory_profiler_cls: Type[pyspark.profiler.MemoryProfiler] = <class 'pyspark.profiler.MemoryProfiler'>)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  accumulator(self, value: ~T, accum_param: Optional[ForwardRef('AccumulatorParam[T]')] = None) -> 'Accumulator[T]'\n",
            " |      Create an :class:`Accumulator` with the given initial value, using a given\n",
            " |      :class:`AccumulatorParam` helper object to define how to add values of the\n",
            " |      data type if provided. Default AccumulatorParams are used for integers\n",
            " |      and floating-point numbers if you do not provide one. For other types,\n",
            " |      a custom AccumulatorParam can be used.\n",
            " |      \n",
            " |      .. versionadded:: 0.7.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      value : T\n",
            " |          initialized value\n",
            " |      accum_param : :class:`pyspark.AccumulatorParam`, optional\n",
            " |          helper object to define how to add values\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`Accumulator`\n",
            " |          `Accumulator` object, a shared variable that can be accumulated\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> acc = sc.accumulator(9)\n",
            " |      >>> acc.value\n",
            " |      9\n",
            " |      >>> acc += 1\n",
            " |      >>> acc.value\n",
            " |      10\n",
            " |      \n",
            " |      Accumulator object can be accumulated in RDD operations:\n",
            " |      \n",
            " |      >>> rdd = sc.range(5)\n",
            " |      >>> def f(x):\n",
            " |      ...     global acc\n",
            " |      ...     acc += 1\n",
            " |      ...\n",
            " |      >>> rdd.foreach(f)\n",
            " |      >>> acc.value\n",
            " |      15\n",
            " |  \n",
            " |  addArchive(self, path: str) -> None\n",
            " |      Add an archive to be downloaded with this Spark job on every node.\n",
            " |      The `path` passed can be either a local file, a file in HDFS\n",
            " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
            " |      FTP URI.\n",
            " |      \n",
            " |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n",
            " |      filename to find its download/unpacked location. The given path should\n",
            " |      be one of .zip, .tar, .tar.gz, .tgz and .jar.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          can be either a local file, a file in HDFS (or other Hadoop-supported\n",
            " |          filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n",
            " |          use :meth:`SparkFiles.get` to find its download location.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.listArchives`\n",
            " |      :meth:`SparkFiles.get`\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      A path can be added only once. Subsequent additions of the same path are ignored.\n",
            " |      This API is experimental.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Creates a zipped file that contains a text file written '100'.\n",
            " |      \n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      >>> import zipfile\n",
            " |      >>> from pyspark import SparkFiles\n",
            " |      \n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     path = os.path.join(d, \"test.txt\")\n",
            " |      ...     with open(path, \"w\") as f:\n",
            " |      ...         _ = f.write(\"100\")\n",
            " |      ...\n",
            " |      ...     zip_path1 = os.path.join(d, \"test1.zip\")\n",
            " |      ...     with zipfile.ZipFile(zip_path1, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
            " |      ...         z.write(path, os.path.basename(path))\n",
            " |      ...\n",
            " |      ...     zip_path2 = os.path.join(d, \"test2.zip\")\n",
            " |      ...     with zipfile.ZipFile(zip_path2, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
            " |      ...         z.write(path, os.path.basename(path))\n",
            " |      ...\n",
            " |      ...     sc.addArchive(zip_path1)\n",
            " |      ...     arch_list1 = sorted(sc.listArchives)\n",
            " |      ...\n",
            " |      ...     sc.addArchive(zip_path2)\n",
            " |      ...     arch_list2 = sorted(sc.listArchives)\n",
            " |      ...\n",
            " |      ...     # add zip_path2 twice, this addition will be ignored\n",
            " |      ...     sc.addArchive(zip_path2)\n",
            " |      ...     arch_list3 = sorted(sc.listArchives)\n",
            " |      ...\n",
            " |      ...     def func(iterator):\n",
            " |      ...         with open(\"%s/test.txt\" % SparkFiles.get(\"test1.zip\")) as f:\n",
            " |      ...             mul = int(f.readline())\n",
            " |      ...             return [x * mul for x in iterator]\n",
            " |      ...\n",
            " |      ...     collected = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
            " |      \n",
            " |      >>> arch_list1\n",
            " |      ['file:/.../test1.zip']\n",
            " |      >>> arch_list2\n",
            " |      ['file:/.../test1.zip', 'file:/.../test2.zip']\n",
            " |      >>> arch_list3\n",
            " |      ['file:/.../test1.zip', 'file:/.../test2.zip']\n",
            " |      >>> collected\n",
            " |      [100, 200, 300, 400]\n",
            " |  \n",
            " |  addFile(self, path: str, recursive: bool = False) -> None\n",
            " |      Add a file to be downloaded with this Spark job on every node.\n",
            " |      The `path` passed can be either a local file, a file in HDFS\n",
            " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
            " |      FTP URI.\n",
            " |      \n",
            " |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n",
            " |      filename to find its download location.\n",
            " |      \n",
            " |      A directory can be given if the recursive option is set to True.\n",
            " |      Currently directories are only supported for Hadoop-supported filesystems.\n",
            " |      \n",
            " |      .. versionadded:: 0.7.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          can be either a local file, a file in HDFS (or other Hadoop-supported\n",
            " |          filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n",
            " |          use :meth:`SparkFiles.get` to find its download location.\n",
            " |      recursive : bool, default False\n",
            " |          whether to recursively add files in the input directory\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.listFiles`\n",
            " |      :meth:`SparkContext.addPyFile`\n",
            " |      :meth:`SparkFiles.get`\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      A path can be added only once. Subsequent additions of the same path are ignored.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      >>> from pyspark import SparkFiles\n",
            " |      \n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     path1 = os.path.join(d, \"test1.txt\")\n",
            " |      ...     with open(path1, \"w\") as f:\n",
            " |      ...         _ = f.write(\"100\")\n",
            " |      ...\n",
            " |      ...     path2 = os.path.join(d, \"test2.txt\")\n",
            " |      ...     with open(path2, \"w\") as f:\n",
            " |      ...         _ = f.write(\"200\")\n",
            " |      ...\n",
            " |      ...     sc.addFile(path1)\n",
            " |      ...     file_list1 = sorted(sc.listFiles)\n",
            " |      ...\n",
            " |      ...     sc.addFile(path2)\n",
            " |      ...     file_list2 = sorted(sc.listFiles)\n",
            " |      ...\n",
            " |      ...     # add path2 twice, this addition will be ignored\n",
            " |      ...     sc.addFile(path2)\n",
            " |      ...     file_list3 = sorted(sc.listFiles)\n",
            " |      ...\n",
            " |      ...     def func(iterator):\n",
            " |      ...         with open(SparkFiles.get(\"test1.txt\")) as f:\n",
            " |      ...             mul = int(f.readline())\n",
            " |      ...             return [x * mul for x in iterator]\n",
            " |      ...\n",
            " |      ...     collected = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
            " |      \n",
            " |      >>> file_list1\n",
            " |      ['file:/.../test1.txt']\n",
            " |      >>> file_list2\n",
            " |      ['file:/.../test1.txt', 'file:/.../test2.txt']\n",
            " |      >>> file_list3\n",
            " |      ['file:/.../test1.txt', 'file:/.../test2.txt']\n",
            " |      >>> collected\n",
            " |      [100, 200, 300, 400]\n",
            " |  \n",
            " |  addJobTag(self, tag: str) -> None\n",
            " |      Add a tag to be assigned to all the jobs started by this thread.\n",
            " |      \n",
            " |      .. versionadded:: 3.5.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      tag : str\n",
            " |          The tag to be added. Cannot contain ',' (comma) character.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.removeJobTag`\n",
            " |      :meth:`SparkContext.getJobTags`\n",
            " |      :meth:`SparkContext.clearJobTags`\n",
            " |      :meth:`SparkContext.cancelJobsWithTag`\n",
            " |      :meth:`SparkContext.setInterruptOnCancel`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import threading\n",
            " |      >>> from time import sleep\n",
            " |      >>> from pyspark import InheritableThread\n",
            " |      >>> sc.setInterruptOnCancel(interruptOnCancel=True)\n",
            " |      >>> result = \"Not Set\"\n",
            " |      >>> lock = threading.Lock()\n",
            " |      >>> def map_func(x):\n",
            " |      ...     sleep(100)\n",
            " |      ...     raise RuntimeError(\"Task should have been cancelled\")\n",
            " |      ...\n",
            " |      >>> def start_job(x):\n",
            " |      ...     global result\n",
            " |      ...     try:\n",
            " |      ...         sc.addJobTag(\"job_to_cancel\")\n",
            " |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
            " |      ...     except Exception as e:\n",
            " |      ...         result = \"Cancelled\"\n",
            " |      ...     lock.release()\n",
            " |      ...\n",
            " |      >>> def stop_job():\n",
            " |      ...     sleep(5)\n",
            " |      ...     sc.cancelJobsWithTag(\"job_to_cancel\")\n",
            " |      ...\n",
            " |      >>> suppress = lock.acquire()\n",
            " |      >>> suppress = InheritableThread(target=start_job, args=(10,)).start()\n",
            " |      >>> suppress = InheritableThread(target=stop_job).start()\n",
            " |      >>> suppress = lock.acquire()\n",
            " |      >>> print(result)\n",
            " |      Cancelled\n",
            " |      >>> sc.clearJobTags()\n",
            " |  \n",
            " |  addPyFile(self, path: str) -> None\n",
            " |      Add a .py or .zip dependency for all tasks to be executed on this\n",
            " |      SparkContext in the future.  The `path` passed can be either a local\n",
            " |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
            " |      HTTP, HTTPS or FTP URI.\n",
            " |      \n",
            " |      .. versionadded:: 0.7.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          can be either a .py file or .zip dependency.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.addFile`\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      A path can be added only once. Subsequent additions of the same path are ignored.\n",
            " |  \n",
            " |  binaryFiles(self, path: str, minPartitions: Optional[int] = None) -> pyspark.rdd.RDD[typing.Tuple[str, bytes]]\n",
            " |      Read a directory of binary files from HDFS, a local file system\n",
            " |      (available on all nodes), or any Hadoop-supported file system URI\n",
            " |      as a byte array. Each file is read as a single record and returned\n",
            " |      in a key-value pair, where the key is the path of each file, the\n",
            " |      value is the content of each file.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          directory to the input data files, the path can be comma separated\n",
            " |          paths as a list of inputs\n",
            " |      minPartitions : int, optional\n",
            " |          suggested minimum number of partitions for the resulting RDD\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          RDD representing path-content pairs from the file(s).\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Small files are preferred, large file is also allowable, but may cause bad performance.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.binaryRecords`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a temporary binary file\n",
            " |      ...     with open(os.path.join(d, \"1.bin\"), \"wb\") as f1:\n",
            " |      ...         _ = f1.write(b\"binary data I\")\n",
            " |      ...\n",
            " |      ...     # Write another temporary binary file\n",
            " |      ...     with open(os.path.join(d, \"2.bin\"), \"wb\") as f2:\n",
            " |      ...         _ = f2.write(b\"binary data II\")\n",
            " |      ...\n",
            " |      ...     collected = sorted(sc.binaryFiles(d).collect())\n",
            " |      \n",
            " |      >>> collected\n",
            " |      [('.../1.bin', b'binary data I'), ('.../2.bin', b'binary data II')]\n",
            " |  \n",
            " |  binaryRecords(self, path: str, recordLength: int) -> pyspark.rdd.RDD[bytes]\n",
            " |      Load data from a flat binary file, assuming each record is a set of numbers\n",
            " |      with the specified numerical format (see ByteBuffer), and the number of\n",
            " |      bytes per record is constant.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          Directory to the input data files\n",
            " |      recordLength : int\n",
            " |          The length at which to split the records\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          RDD of data with values, represented as byte arrays\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.binaryFiles`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a temporary file\n",
            " |      ...     with open(os.path.join(d, \"1.bin\"), \"w\") as f:\n",
            " |      ...         for i in range(3):\n",
            " |      ...             _ = f.write(\"%04d\" % i)\n",
            " |      ...\n",
            " |      ...     # Write another file\n",
            " |      ...     with open(os.path.join(d, \"2.bin\"), \"w\") as f:\n",
            " |      ...         for i in [-1, -2, -10]:\n",
            " |      ...             _ = f.write(\"%04d\" % i)\n",
            " |      ...\n",
            " |      ...     collected = sorted(sc.binaryRecords(d, 4).collect())\n",
            " |      \n",
            " |      >>> collected\n",
            " |      [b'-001', b'-002', b'-010', b'0000', b'0001', b'0002']\n",
            " |  \n",
            " |  broadcast(self, value: ~T) -> 'Broadcast[T]'\n",
            " |      Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\n",
            " |      object for reading it in distributed functions. The variable will\n",
            " |      be sent to each cluster only once.\n",
            " |      \n",
            " |      .. versionadded:: 0.7.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      value : T\n",
            " |          value to broadcast to the Spark nodes\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`Broadcast`\n",
            " |          :class:`Broadcast` object, a read-only variable cached on each machine\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> mapping = {1: 10001, 2: 10002}\n",
            " |      >>> bc = sc.broadcast(mapping)\n",
            " |      \n",
            " |      >>> rdd = sc.range(5)\n",
            " |      >>> rdd2 = rdd.map(lambda i: bc.value[i] if i in bc.value else -1)\n",
            " |      >>> rdd2.collect()\n",
            " |      [-1, 10001, 10002, -1, -1]\n",
            " |      \n",
            " |      >>> bc.destroy()\n",
            " |  \n",
            " |  cancelAllJobs(self) -> None\n",
            " |      Cancel all jobs that have been scheduled or are running.\n",
            " |      \n",
            " |      .. versionadded:: 1.1.0\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.cancelJobGroup`\n",
            " |      :meth:`SparkContext.cancelJobsWithTag`\n",
            " |      :meth:`SparkContext.runJob`\n",
            " |  \n",
            " |  cancelJobGroup(self, groupId: str) -> None\n",
            " |      Cancel active jobs for the specified group. See :meth:`SparkContext.setJobGroup`.\n",
            " |      for more information.\n",
            " |      \n",
            " |      .. versionadded:: 1.1.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      groupId : str\n",
            " |          The group ID to cancel the job.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.setJobGroup`\n",
            " |  \n",
            " |  cancelJobsWithTag(self, tag: str) -> None\n",
            " |      Cancel active jobs that have the specified tag. See\n",
            " |      :meth:`SparkContext.addJobTag`.\n",
            " |      \n",
            " |      .. versionadded:: 3.5.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      tag : str\n",
            " |          The tag to be cancelled. Cannot contain ',' (comma) character.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.addJobTag`\n",
            " |      :meth:`SparkContext.removeJobTag`\n",
            " |      :meth:`SparkContext.getJobTags`\n",
            " |      :meth:`SparkContext.clearJobTags`\n",
            " |      :meth:`SparkContext.setInterruptOnCancel`\n",
            " |  \n",
            " |  clearJobTags(self) -> None\n",
            " |      Clear the current thread's job tags.\n",
            " |      \n",
            " |      .. versionadded:: 3.5.0\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.addJobTag`\n",
            " |      :meth:`SparkContext.removeJobTag`\n",
            " |      :meth:`SparkContext.getJobTags`\n",
            " |      :meth:`SparkContext.cancelJobsWithTag`\n",
            " |      :meth:`SparkContext.setInterruptOnCancel`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.addJobTag(\"job_to_cancel\")\n",
            " |      >>> sc.clearJobTags()\n",
            " |      >>> sc.getJobTags()\n",
            " |      set()\n",
            " |  \n",
            " |  dump_profiles(self, path: str) -> None\n",
            " |      Dump the profile stats into directory `path`\n",
            " |      \n",
            " |      .. versionadded:: 1.2.0\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.show_profiles`\n",
            " |  \n",
            " |  emptyRDD(self) -> pyspark.rdd.RDD[typing.Any]\n",
            " |      Create an :class:`RDD` that has no partitions or elements.\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          An empty RDD\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.emptyRDD()\n",
            " |      EmptyRDD...\n",
            " |      >>> sc.emptyRDD().count()\n",
            " |      0\n",
            " |  \n",
            " |  getCheckpointDir(self) -> Optional[str]\n",
            " |      Return the directory where RDDs are checkpointed. Returns None if no\n",
            " |      checkpoint directory has been set.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.setCheckpointDir`\n",
            " |      :meth:`RDD.checkpoint`\n",
            " |      :meth:`RDD.getCheckpointFile`\n",
            " |  \n",
            " |  getConf(self) -> pyspark.conf.SparkConf\n",
            " |      Return a copy of this SparkContext's configuration :class:`SparkConf`.\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |  \n",
            " |  getJobTags(self) -> Set[str]\n",
            " |      Get the tags that are currently set to be assigned to all the jobs started by this thread.\n",
            " |      \n",
            " |      .. versionadded:: 3.5.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      set of str\n",
            " |          the tags that are currently set to be assigned to all the jobs started by this thread.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.addJobTag`\n",
            " |      :meth:`SparkContext.removeJobTag`\n",
            " |      :meth:`SparkContext.clearJobTags`\n",
            " |      :meth:`SparkContext.cancelJobsWithTag`\n",
            " |      :meth:`SparkContext.setInterruptOnCancel`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.addJobTag(\"job_to_cancel\")\n",
            " |      >>> sc.getJobTags()\n",
            " |      {'job_to_cancel'}\n",
            " |      >>> sc.clearJobTags()\n",
            " |  \n",
            " |  getLocalProperty(self, key: str) -> Optional[str]\n",
            " |      Get a local property set in this thread, or null if it is missing. See\n",
            " |      :meth:`setLocalProperty`.\n",
            " |      \n",
            " |      .. versionadded:: 1.0.0\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.setLocalProperty`\n",
            " |  \n",
            " |  hadoopFile(self, path: str, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
            " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
            " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
            " |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
            " |      \n",
            " |      .. versionadded:: 1.1.0\n",
            " |      \n",
            " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
            " |      Configuration in Java.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          path to Hadoop file\n",
            " |      inputFormatClass : str\n",
            " |          fully qualified classname of Hadoop InputFormat\n",
            " |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
            " |      keyClass : str\n",
            " |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
            " |      valueClass : str\n",
            " |          fully qualified classname of value Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified name of a function returning key WritableConverter\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified name of a function returning value WritableConverter\n",
            " |      conf : dict, optional\n",
            " |          Hadoop configuration, passed in as a dict\n",
            " |      batchSize : int, optional, default 0\n",
            " |          The number of Python objects represented as a single\n",
            " |          Java object. (default 0, choose batchSize automatically)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          RDD of tuples of key and corresponding value\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`RDD.saveAsSequenceFile`\n",
            " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
            " |      :meth:`RDD.saveAsHadoopFile`\n",
            " |      :meth:`SparkContext.newAPIHadoopFile`\n",
            " |      :meth:`SparkContext.hadoopRDD`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      \n",
            " |      Set the related classes\n",
            " |      \n",
            " |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
            " |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
            " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
            " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
            " |      \n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
            " |      ...\n",
            " |      ...     # Write a temporary Hadoop file\n",
            " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
            " |      ...     rdd.saveAsHadoopFile(path, output_format_class, key_class, value_class)\n",
            " |      ...\n",
            " |      ...     loaded = sc.hadoopFile(path, input_format_class, key_class, value_class)\n",
            " |      ...     collected = sorted(loaded.collect())\n",
            " |      \n",
            " |      >>> collected\n",
            " |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
            " |  \n",
            " |  hadoopRDD(self, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
            " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
            " |      Hadoop configuration, which is passed in as a Python dict.\n",
            " |      This will be converted into a Configuration in Java.\n",
            " |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
            " |      \n",
            " |      .. versionadded:: 1.1.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      inputFormatClass : str\n",
            " |          fully qualified classname of Hadoop InputFormat\n",
            " |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
            " |      keyClass : str\n",
            " |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
            " |      valueClass : str\n",
            " |          fully qualified classname of value Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified name of a function returning key WritableConverter\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified name of a function returning value WritableConverter\n",
            " |      conf : dict, optional\n",
            " |          Hadoop configuration, passed in as a dict\n",
            " |      batchSize : int, optional, default 0\n",
            " |          The number of Python objects represented as a single\n",
            " |          Java object. (default 0, choose batchSize automatically)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          RDD of tuples of key and corresponding value\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
            " |      :meth:`RDD.saveAsHadoopDataset`\n",
            " |      :meth:`SparkContext.newAPIHadoopRDD`\n",
            " |      :meth:`SparkContext.hadoopFile`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      \n",
            " |      Set the related classes\n",
            " |      \n",
            " |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
            " |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
            " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
            " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
            " |      \n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
            " |      ...\n",
            " |      ...     # Create the conf for writing\n",
            " |      ...     write_conf = {\n",
            " |      ...         \"mapred.output.format.class\": output_format_class,\n",
            " |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
            " |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
            " |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
            " |      ...     }\n",
            " |      ...\n",
            " |      ...     # Write a temporary Hadoop file\n",
            " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
            " |      ...     rdd.saveAsHadoopDataset(conf=write_conf)\n",
            " |      ...\n",
            " |      ...     # Create the conf for reading\n",
            " |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
            " |      ...\n",
            " |      ...     loaded = sc.hadoopRDD(input_format_class, key_class, value_class, conf=read_conf)\n",
            " |      ...     collected = sorted(loaded.collect())\n",
            " |      \n",
            " |      >>> collected\n",
            " |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
            " |  \n",
            " |  newAPIHadoopFile(self, path: str, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
            " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
            " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
            " |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
            " |      \n",
            " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
            " |      Configuration in Java\n",
            " |      \n",
            " |      .. versionadded:: 1.1.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          path to Hadoop file\n",
            " |      inputFormatClass : str\n",
            " |          fully qualified classname of Hadoop InputFormat\n",
            " |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
            " |      keyClass : str\n",
            " |          fully qualified classname of key Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.Text\")\n",
            " |      valueClass : str\n",
            " |          fully qualified classname of value Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified name of a function returning key WritableConverter\n",
            " |          None by default\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified name of a function returning value WritableConverter\n",
            " |          None by default\n",
            " |      conf : dict, optional\n",
            " |          Hadoop configuration, passed in as a dict\n",
            " |          None by default\n",
            " |      batchSize : int, optional, default 0\n",
            " |          The number of Python objects represented as a single\n",
            " |          Java object. (default 0, choose batchSize automatically)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          RDD of tuples of key and corresponding value\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`RDD.saveAsSequenceFile`\n",
            " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
            " |      :meth:`RDD.saveAsHadoopFile`\n",
            " |      :meth:`SparkContext.sequenceFile`\n",
            " |      :meth:`SparkContext.hadoopFile`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      \n",
            " |      Set the related classes\n",
            " |      \n",
            " |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
            " |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n",
            " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
            " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
            " |      \n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     path = os.path.join(d, \"new_hadoop_file\")\n",
            " |      ...\n",
            " |      ...     # Write a temporary Hadoop file\n",
            " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
            " |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class, key_class, value_class)\n",
            " |      ...\n",
            " |      ...     loaded = sc.newAPIHadoopFile(path, input_format_class, key_class, value_class)\n",
            " |      ...     collected = sorted(loaded.collect())\n",
            " |      \n",
            " |      >>> collected\n",
            " |      [(1, ''), (1, 'a'), (3, 'x')]\n",
            " |  \n",
            " |  newAPIHadoopRDD(self, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
            " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
            " |      Hadoop configuration, which is passed in as a Python dict.\n",
            " |      This will be converted into a Configuration in Java.\n",
            " |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
            " |      \n",
            " |      .. versionadded:: 1.1.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      inputFormatClass : str\n",
            " |          fully qualified classname of Hadoop InputFormat\n",
            " |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
            " |      keyClass : str\n",
            " |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
            " |      valueClass : str\n",
            " |          fully qualified classname of value Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified name of a function returning key WritableConverter\n",
            " |          (None by default)\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified name of a function returning value WritableConverter\n",
            " |          (None by default)\n",
            " |      conf : dict, optional\n",
            " |          Hadoop configuration, passed in as a dict (None by default)\n",
            " |      batchSize : int, optional, default 0\n",
            " |          The number of Python objects represented as a single\n",
            " |          Java object. (default 0, choose batchSize automatically)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          RDD of tuples of key and corresponding value\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
            " |      :meth:`RDD.saveAsHadoopDataset`\n",
            " |      :meth:`SparkContext.hadoopRDD`\n",
            " |      :meth:`SparkContext.hadoopFile`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      \n",
            " |      Set the related classes\n",
            " |      \n",
            " |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
            " |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n",
            " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
            " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
            " |      \n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     path = os.path.join(d, \"new_hadoop_file\")\n",
            " |      ...\n",
            " |      ...     # Create the conf for writing\n",
            " |      ...     write_conf = {\n",
            " |      ...         \"mapreduce.job.outputformat.class\": (output_format_class),\n",
            " |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
            " |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
            " |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
            " |      ...     }\n",
            " |      ...\n",
            " |      ...     # Write a temporary Hadoop file\n",
            " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
            " |      ...     rdd.saveAsNewAPIHadoopDataset(conf=write_conf)\n",
            " |      ...\n",
            " |      ...     # Create the conf for reading\n",
            " |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
            " |      ...\n",
            " |      ...     loaded = sc.newAPIHadoopRDD(input_format_class,\n",
            " |      ...         key_class, value_class, conf=read_conf)\n",
            " |      ...     collected = sorted(loaded.collect())\n",
            " |      \n",
            " |      >>> collected\n",
            " |      [(1, ''), (1, 'a'), (3, 'x')]\n",
            " |  \n",
            " |  parallelize(self, c: Iterable[~T], numSlices: Optional[int] = None) -> pyspark.rdd.RDD[~T]\n",
            " |      Distribute a local Python collection to form an RDD. Using range\n",
            " |      is recommended if the input represents a range for performance.\n",
            " |      \n",
            " |      .. versionadded:: 0.7.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      c : :class:`collections.abc.Iterable`\n",
            " |          iterable collection to distribute\n",
            " |      numSlices : int, optional\n",
            " |          the number of partitions of the new RDD\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          RDD representing distributed collection.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
            " |      [[0], [2], [3], [4], [6]]\n",
            " |      >>> sc.parallelize(range(0, 6, 2), 5).glom().collect()\n",
            " |      [[], [0], [], [2], [4]]\n",
            " |      \n",
            " |      Deal with a list of strings.\n",
            " |      \n",
            " |      >>> strings = [\"a\", \"b\", \"c\"]\n",
            " |      >>> sc.parallelize(strings, 2).glom().collect()\n",
            " |      [['a'], ['b', 'c']]\n",
            " |  \n",
            " |  pickleFile(self, name: str, minPartitions: Optional[int] = None) -> pyspark.rdd.RDD[typing.Any]\n",
            " |      Load an RDD previously saved using :meth:`RDD.saveAsPickleFile` method.\n",
            " |      \n",
            " |      .. versionadded:: 1.1.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          directory to the input data files, the path can be comma separated\n",
            " |          paths as a list of inputs\n",
            " |      minPartitions : int, optional\n",
            " |          suggested minimum number of partitions for the resulting RDD\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          RDD representing unpickled data from the file(s).\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`RDD.saveAsPickleFile`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a temporary pickled file\n",
            " |      ...     path1 = os.path.join(d, \"pickled1\")\n",
            " |      ...     sc.parallelize(range(10)).saveAsPickleFile(path1, 3)\n",
            " |      ...\n",
            " |      ...     # Write another temporary pickled file\n",
            " |      ...     path2 = os.path.join(d, \"pickled2\")\n",
            " |      ...     sc.parallelize(range(-10, -5)).saveAsPickleFile(path2, 3)\n",
            " |      ...\n",
            " |      ...     # Load picked file\n",
            " |      ...     collected1 = sorted(sc.pickleFile(path1, 3).collect())\n",
            " |      ...     collected2 = sorted(sc.pickleFile(path2, 4).collect())\n",
            " |      ...\n",
            " |      ...     # Load two picked files together\n",
            " |      ...     collected3 = sorted(sc.pickleFile('{},{}'.format(path1, path2), 5).collect())\n",
            " |      \n",
            " |      >>> collected1\n",
            " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            " |      >>> collected2\n",
            " |      [-10, -9, -8, -7, -6]\n",
            " |      >>> collected3\n",
            " |      [-10, -9, -8, -7, -6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            " |  \n",
            " |  range(self, start: int, end: Optional[int] = None, step: int = 1, numSlices: Optional[int] = None) -> pyspark.rdd.RDD[int]\n",
            " |      Create a new RDD of int containing elements from `start` to `end`\n",
            " |      (exclusive), increased by `step` every element. Can be called the same\n",
            " |      way as python's built-in range() function. If called with a single argument,\n",
            " |      the argument is interpreted as `end`, and `start` is set to 0.\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      start : int\n",
            " |          the start value\n",
            " |      end : int, optional\n",
            " |          the end value (exclusive)\n",
            " |      step : int, optional, default 1\n",
            " |          the incremental step\n",
            " |      numSlices : int, optional\n",
            " |          the number of partitions of the new RDD\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          An RDD of int\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`pyspark.sql.SparkSession.range`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.range(5).collect()\n",
            " |      [0, 1, 2, 3, 4]\n",
            " |      >>> sc.range(2, 4).collect()\n",
            " |      [2, 3]\n",
            " |      >>> sc.range(1, 7, 2).collect()\n",
            " |      [1, 3, 5]\n",
            " |      \n",
            " |      Generate RDD with a negative step\n",
            " |      \n",
            " |      >>> sc.range(5, 0, -1).collect()\n",
            " |      [5, 4, 3, 2, 1]\n",
            " |      >>> sc.range(0, 5, -1).collect()\n",
            " |      []\n",
            " |      \n",
            " |      Control the number of partitions\n",
            " |      \n",
            " |      >>> sc.range(5, numSlices=1).getNumPartitions()\n",
            " |      1\n",
            " |      >>> sc.range(5, numSlices=10).getNumPartitions()\n",
            " |      10\n",
            " |  \n",
            " |  removeJobTag(self, tag: str) -> None\n",
            " |      Remove a tag previously added to be assigned to all the jobs started by this thread.\n",
            " |      Noop if such a tag was not added earlier.\n",
            " |      \n",
            " |      .. versionadded:: 3.5.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      tag : str\n",
            " |          The tag to be removed. Cannot contain ',' (comma) character.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.addJobTag`\n",
            " |      :meth:`SparkContext.getJobTags`\n",
            " |      :meth:`SparkContext.clearJobTags`\n",
            " |      :meth:`SparkContext.cancelJobsWithTag`\n",
            " |      :meth:`SparkContext.setInterruptOnCancel`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.addJobTag(\"job_to_cancel1\")\n",
            " |      >>> sc.addJobTag(\"job_to_cancel2\")\n",
            " |      >>> sc.getJobTags()\n",
            " |      {'job_to_cancel1', 'job_to_cancel2'}\n",
            " |      >>> sc.removeJobTag(\"job_to_cancel1\")\n",
            " |      >>> sc.getJobTags()\n",
            " |      {'job_to_cancel2'}\n",
            " |      >>> sc.clearJobTags()\n",
            " |  \n",
            " |  runJob(self, rdd: pyspark.rdd.RDD[~T], partitionFunc: Callable[[Iterable[~T]], Iterable[~U]], partitions: Optional[Sequence[int]] = None, allowLocal: bool = False) -> List[~U]\n",
            " |      Executes the given partitionFunc on the specified set of partitions,\n",
            " |      returning the result as an array of elements.\n",
            " |      \n",
            " |      If 'partitions' is not specified, this will run over all partitions.\n",
            " |      \n",
            " |      .. versionadded:: 1.1.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      rdd : :class:`RDD`\n",
            " |          target RDD to run tasks on\n",
            " |      partitionFunc : function\n",
            " |          a function to run on each partition of the RDD\n",
            " |      partitions : list, optional\n",
            " |          set of partitions to run on; some jobs may not want to compute on all\n",
            " |          partitions of the target RDD, e.g. for operations like `first`\n",
            " |      allowLocal : bool, default False\n",
            " |          this parameter takes no effect\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          results of specified partitions\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.cancelAllJobs`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
            " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
            " |      [0, 1, 4, 9, 16, 25]\n",
            " |      \n",
            " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
            " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
            " |      [0, 1, 16, 25]\n",
            " |  \n",
            " |  sequenceFile(self, path: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, minSplits: Optional[int] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
            " |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
            " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
            " |      The mechanism is as follows:\n",
            " |      \n",
            " |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
            " |             and value Writable classes\n",
            " |          2. Serialization is attempted via Pickle pickling\n",
            " |          3. If this fails, the fallback is to call 'toString' on each key and value\n",
            " |          4. :class:`CPickleSerializer` is used to deserialize pickled objects on the Python side\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          path to sequencefile\n",
            " |      keyClass: str, optional\n",
            " |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
            " |      valueClass : str, optional\n",
            " |          fully qualified classname of value Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified name of a function returning key WritableConverter\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualifiedname of a function returning value WritableConverter\n",
            " |      minSplits : int, optional\n",
            " |          minimum splits in dataset (default min(2, sc.defaultParallelism))\n",
            " |      batchSize : int, optional, default 0\n",
            " |          The number of Python objects represented as a single\n",
            " |          Java object. (default 0, choose batchSize automatically)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          RDD of tuples of key and corresponding value\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`RDD.saveAsSequenceFile`\n",
            " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
            " |      :meth:`RDD.saveAsHadoopFile`\n",
            " |      :meth:`SparkContext.newAPIHadoopFile`\n",
            " |      :meth:`SparkContext.hadoopFile`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      \n",
            " |      Set the class of output format\n",
            " |      \n",
            " |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
            " |      \n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     path = os.path.join(d, \"hadoop_file\")\n",
            " |      ...\n",
            " |      ...     # Write a temporary Hadoop file\n",
            " |      ...     rdd = sc.parallelize([(1, {3.0: \"bb\"}), (2, {1.0: \"aa\"}), (3, {2.0: \"dd\"})])\n",
            " |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class)\n",
            " |      ...\n",
            " |      ...     collected = sorted(sc.sequenceFile(path).collect())\n",
            " |      \n",
            " |      >>> collected\n",
            " |      [(1, {3.0: 'bb'}), (2, {1.0: 'aa'}), (3, {2.0: 'dd'})]\n",
            " |  \n",
            " |  setCheckpointDir(self, dirName: str) -> None\n",
            " |      Set the directory under which RDDs are going to be checkpointed. The\n",
            " |      directory must be an HDFS path if running on a cluster.\n",
            " |      \n",
            " |      .. versionadded:: 0.7.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      dirName : str\n",
            " |          path to the directory where checkpoint files will be stored\n",
            " |          (must be HDFS path if running in cluster)\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.getCheckpointDir`\n",
            " |      :meth:`RDD.checkpoint`\n",
            " |      :meth:`RDD.getCheckpointFile`\n",
            " |  \n",
            " |  setInterruptOnCancel(self, interruptOnCancel: bool) -> None\n",
            " |      Set the behavior of job cancellation from jobs started in this thread.\n",
            " |      \n",
            " |      .. versionadded:: 3.5.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      interruptOnCancel : bool\n",
            " |          If true, then job cancellation will result in ``Thread.interrupt()``\n",
            " |          being called on the job's executor threads. This is useful to help ensure that\n",
            " |          the tasks are actually stopped in a timely manner, but is off by default due to\n",
            " |          HDFS-1208, where HDFS may respond to ``Thread.interrupt()`` by marking nodes as dead.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.addJobTag`\n",
            " |      :meth:`SparkContext.removeJobTag`\n",
            " |      :meth:`SparkContext.cancelAllJobs`\n",
            " |      :meth:`SparkContext.cancelJobGroup`\n",
            " |      :meth:`SparkContext.cancelJobsWithTag`\n",
            " |  \n",
            " |  setJobDescription(self, value: str) -> None\n",
            " |      Set a human readable description of the current job.\n",
            " |      \n",
            " |      .. versionadded:: 2.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      value : str\n",
            " |          The job description to set.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n",
            " |      local inheritance.\n",
            " |  \n",
            " |  setJobGroup(self, groupId: str, description: str, interruptOnCancel: bool = False) -> None\n",
            " |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
            " |      different value or cleared.\n",
            " |      \n",
            " |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
            " |      Application programmers can use this method to group all those jobs together and give a\n",
            " |      group description. Once set, the Spark web UI will associate such jobs with this group.\n",
            " |      \n",
            " |      The application can use :meth:`SparkContext.cancelJobGroup` to cancel all\n",
            " |      running jobs in this group.\n",
            " |      \n",
            " |      .. versionadded:: 1.0.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      groupId : str\n",
            " |          The group ID to assign.\n",
            " |      description : str\n",
            " |          The description to set for the job group.\n",
            " |      interruptOnCancel : bool, optional, default False\n",
            " |          whether to interrupt jobs on job cancellation.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
            " |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
            " |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
            " |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
            " |      \n",
            " |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n",
            " |      local inheritance.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.cancelJobGroup`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import threading\n",
            " |      >>> from time import sleep\n",
            " |      >>> from pyspark import InheritableThread\n",
            " |      >>> result = \"Not Set\"\n",
            " |      >>> lock = threading.Lock()\n",
            " |      >>> def map_func(x):\n",
            " |      ...     sleep(100)\n",
            " |      ...     raise RuntimeError(\"Task should have been cancelled\")\n",
            " |      ...\n",
            " |      >>> def start_job(x):\n",
            " |      ...     global result\n",
            " |      ...     try:\n",
            " |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
            " |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
            " |      ...     except Exception as e:\n",
            " |      ...         result = \"Cancelled\"\n",
            " |      ...     lock.release()\n",
            " |      ...\n",
            " |      >>> def stop_job():\n",
            " |      ...     sleep(5)\n",
            " |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
            " |      ...\n",
            " |      >>> suppress = lock.acquire()\n",
            " |      >>> suppress = InheritableThread(target=start_job, args=(10,)).start()\n",
            " |      >>> suppress = InheritableThread(target=stop_job).start()\n",
            " |      >>> suppress = lock.acquire()\n",
            " |      >>> print(result)\n",
            " |      Cancelled\n",
            " |  \n",
            " |  setLocalProperty(self, key: str, value: str) -> None\n",
            " |      Set a local property that affects jobs submitted from this thread, such as the\n",
            " |      Spark fair scheduler pool.\n",
            " |      \n",
            " |      .. versionadded:: 1.0.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key : str\n",
            " |          The key of the local property to set.\n",
            " |      value : str\n",
            " |          The value of the local property to set.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.getLocalProperty`\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n",
            " |      local inheritance.\n",
            " |  \n",
            " |  setLogLevel(self, logLevel: str) -> None\n",
            " |      Control our logLevel. This overrides any user-defined log settings.\n",
            " |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      logLevel : str\n",
            " |          The desired log level as a string.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.setLogLevel(\"WARN\")  # doctest :+SKIP\n",
            " |  \n",
            " |  show_profiles(self) -> None\n",
            " |      Print the profile stats to stdout\n",
            " |      \n",
            " |      .. versionadded:: 1.2.0\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.dump_profiles`\n",
            " |  \n",
            " |  sparkUser(self) -> str\n",
            " |      Get SPARK_USER for user who is running SparkContext.\n",
            " |      \n",
            " |      .. versionadded:: 1.0.0\n",
            " |  \n",
            " |  statusTracker(self) -> pyspark.status.StatusTracker\n",
            " |      Return :class:`StatusTracker` object\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |  \n",
            " |  stop(self) -> None\n",
            " |      Shut down the :class:`SparkContext`.\n",
            " |      \n",
            " |      .. versionadded:: 0.7.0\n",
            " |  \n",
            " |  textFile(self, name: str, minPartitions: Optional[int] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n",
            " |      Read a text file from HDFS, a local file system (available on all\n",
            " |      nodes), or any Hadoop-supported file system URI, and return it as an\n",
            " |      RDD of Strings. The text files must be encoded as UTF-8.\n",
            " |      \n",
            " |      .. versionadded:: 0.7.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          directory to the input data files, the path can be comma separated\n",
            " |          paths as a list of inputs\n",
            " |      minPartitions : int, optional\n",
            " |          suggested minimum number of partitions for the resulting RDD\n",
            " |      use_unicode : bool, default True\n",
            " |          If `use_unicode` is False, the strings will be kept as `str` (encoding\n",
            " |          as `utf-8`), which is faster and smaller than unicode.\n",
            " |      \n",
            " |          .. versionadded:: 1.2.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          RDD representing text data from the file(s).\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`RDD.saveAsTextFile`\n",
            " |      :meth:`SparkContext.wholeTextFiles`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     path1 = os.path.join(d, \"text1\")\n",
            " |      ...     path2 = os.path.join(d, \"text2\")\n",
            " |      ...\n",
            " |      ...     # Write a temporary text file\n",
            " |      ...     sc.parallelize([\"x\", \"y\", \"z\"]).saveAsTextFile(path1)\n",
            " |      ...\n",
            " |      ...     # Write another temporary text file\n",
            " |      ...     sc.parallelize([\"aa\", \"bb\", \"cc\"]).saveAsTextFile(path2)\n",
            " |      ...\n",
            " |      ...     # Load text file\n",
            " |      ...     collected1 = sorted(sc.textFile(path1, 3).collect())\n",
            " |      ...     collected2 = sorted(sc.textFile(path2, 4).collect())\n",
            " |      ...\n",
            " |      ...     # Load two text files together\n",
            " |      ...     collected3 = sorted(sc.textFile('{},{}'.format(path1, path2), 5).collect())\n",
            " |      \n",
            " |      >>> collected1\n",
            " |      ['x', 'y', 'z']\n",
            " |      >>> collected2\n",
            " |      ['aa', 'bb', 'cc']\n",
            " |      >>> collected3\n",
            " |      ['aa', 'bb', 'cc', 'x', 'y', 'z']\n",
            " |  \n",
            " |  union(self, rdds: List[pyspark.rdd.RDD[~T]]) -> pyspark.rdd.RDD[~T]\n",
            " |      Build the union of a list of RDDs.\n",
            " |      \n",
            " |      This supports unions() of RDDs with different serialized formats,\n",
            " |      although this forces them to be reserialized using the default\n",
            " |      serializer:\n",
            " |      \n",
            " |      .. versionadded:: 0.7.0\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`RDD.union`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # generate a text RDD\n",
            " |      ...     with open(os.path.join(d, \"union-text.txt\"), \"w\") as f:\n",
            " |      ...         _ = f.write(\"Hello\")\n",
            " |      ...     text_rdd = sc.textFile(d)\n",
            " |      ...\n",
            " |      ...     # generate another RDD\n",
            " |      ...     parallelized = sc.parallelize([\"World!\"])\n",
            " |      ...\n",
            " |      ...     unioned = sorted(sc.union([text_rdd, parallelized]).collect())\n",
            " |      \n",
            " |      >>> unioned\n",
            " |      ['Hello', 'World!']\n",
            " |  \n",
            " |  wholeTextFiles(self, path: str, minPartitions: Optional[int] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[typing.Tuple[str, str]]\n",
            " |      Read a directory of text files from HDFS, a local file system\n",
            " |      (available on all nodes), or any  Hadoop-supported file system\n",
            " |      URI. Each file is read as a single record and returned in a\n",
            " |      key-value pair, where the key is the path of each file, the\n",
            " |      value is the content of each file.\n",
            " |      The text files must be encoded as UTF-8.\n",
            " |      \n",
            " |      .. versionadded:: 1.0.0\n",
            " |      \n",
            " |      For example, if you have the following files:\n",
            " |      \n",
            " |      .. code-block:: text\n",
            " |      \n",
            " |          hdfs://a-hdfs-path/part-00000\n",
            " |          hdfs://a-hdfs-path/part-00001\n",
            " |          ...\n",
            " |          hdfs://a-hdfs-path/part-nnnnn\n",
            " |      \n",
            " |      Do ``rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")``,\n",
            " |      then ``rdd`` contains:\n",
            " |      \n",
            " |      .. code-block:: text\n",
            " |      \n",
            " |          (a-hdfs-path/part-00000, its content)\n",
            " |          (a-hdfs-path/part-00001, its content)\n",
            " |          ...\n",
            " |          (a-hdfs-path/part-nnnnn, its content)\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          directory to the input data files, the path can be comma separated\n",
            " |          paths as a list of inputs\n",
            " |      minPartitions : int, optional\n",
            " |          suggested minimum number of partitions for the resulting RDD\n",
            " |      use_unicode : bool, default True\n",
            " |          If `use_unicode` is False, the strings will be kept as `str` (encoding\n",
            " |          as `utf-8`), which is faster and smaller than unicode.\n",
            " |      \n",
            " |          .. versionadded:: 1.2.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |          RDD representing path-content pairs from the file(s).\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Small files are preferred, as each file will be loaded fully in memory.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`RDD.saveAsTextFile`\n",
            " |      :meth:`SparkContext.textFile`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import os\n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a temporary text file\n",
            " |      ...     with open(os.path.join(d, \"1.txt\"), \"w\") as f:\n",
            " |      ...         _ = f.write(\"123\")\n",
            " |      ...\n",
            " |      ...     # Write another temporary text file\n",
            " |      ...     with open(os.path.join(d, \"2.txt\"), \"w\") as f:\n",
            " |      ...         _ = f.write(\"xyz\")\n",
            " |      ...\n",
            " |      ...     collected = sorted(sc.wholeTextFiles(d).collect())\n",
            " |      >>> collected\n",
            " |      [('.../1.txt', '123'), ('.../2.txt', 'xyz')]\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  getOrCreate(conf: Optional[pyspark.conf.SparkConf] = None) -> 'SparkContext' from builtins.type\n",
            " |      Get or instantiate a :class:`SparkContext` and register it as a singleton object.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      conf : :class:`SparkConf`, optional\n",
            " |          :class:`SparkConf` that will be used for initialization of the :class:`SparkContext`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`SparkContext`\n",
            " |          current :class:`SparkContext`, or a new one if it wasn't created before the function\n",
            " |          call.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> SparkContext.getOrCreate()\n",
            " |      <SparkContext ...>\n",
            " |  \n",
            " |  setSystemProperty(key: str, value: str) -> None from builtins.type\n",
            " |      Set a Java system property, such as `spark.executor.memory`. This must\n",
            " |      be invoked before instantiating :class:`SparkContext`.\n",
            " |      \n",
            " |      .. versionadded:: 0.9.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key : str\n",
            " |          The key of a new Java system property.\n",
            " |      value : str\n",
            " |          The value of a new Java system property.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  applicationId\n",
            " |      A unique identifier for the Spark application.\n",
            " |      Its format depends on the scheduler implementation.\n",
            " |      \n",
            " |      * in case of local spark app something like 'local-1433865536131'\n",
            " |      * in case of YARN something like 'application_1433865536131_34483'\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.applicationId  # doctest: +ELLIPSIS\n",
            " |      'local-...'\n",
            " |  \n",
            " |  defaultMinPartitions\n",
            " |      Default min number of partitions for Hadoop RDDs when not given by user\n",
            " |      \n",
            " |      .. versionadded:: 1.1.0\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.defaultMinPartitions > 0\n",
            " |      True\n",
            " |  \n",
            " |  defaultParallelism\n",
            " |      Default level of parallelism to use when not given by user (e.g. for reduce tasks)\n",
            " |      \n",
            " |      .. versionadded:: 0.7.0\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.defaultParallelism > 0\n",
            " |      True\n",
            " |  \n",
            " |  listArchives\n",
            " |      Returns a list of archive paths that are added to resources.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.addArchive`\n",
            " |  \n",
            " |  listFiles\n",
            " |      Returns a list of file paths that are added to resources.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`SparkContext.addFile`\n",
            " |  \n",
            " |  resources\n",
            " |      Return the resource information of this :class:`SparkContext`.\n",
            " |      A resource could be a GPU, FPGA, etc.\n",
            " |      \n",
            " |      .. versionadded:: 3.0.0\n",
            " |  \n",
            " |  startTime\n",
            " |      Return the epoch time when the :class:`SparkContext` was started.\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> _ = sc.startTime\n",
            " |  \n",
            " |  uiWebUrl\n",
            " |      Return the URL of the SparkUI instance started by this :class:`SparkContext`\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      When the web ui is disabled, e.g., by ``spark.ui.enabled`` set to ``False``,\n",
            " |      it returns ``None``.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.uiWebUrl\n",
            " |      'http://...'\n",
            " |  \n",
            " |  version\n",
            " |      The version of Spark on which this application is running.\n",
            " |      \n",
            " |      .. versionadded:: 1.1.0\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> _ = sc.version\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n",
            " |  \n",
            " |  __annotations__ = {'PACKAGE_EXTENSIONS': typing.Iterable[str], '_activ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMzt4qnTa_yR",
        "outputId": "34b8eb1c-4e2f-4bb1-f29f-1464f1b464d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['PACKAGE_EXTENSIONS',\n",
              " '__annotations__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__enter__',\n",
              " '__eq__',\n",
              " '__exit__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getnewargs__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_accumulatorServer',\n",
              " '_active_spark_context',\n",
              " '_assert_on_driver',\n",
              " '_batchSize',\n",
              " '_callsite',\n",
              " '_checkpointFile',\n",
              " '_conf',\n",
              " '_dictToJavaMap',\n",
              " '_do_init',\n",
              " '_encryption_enabled',\n",
              " '_ensure_initialized',\n",
              " '_gateway',\n",
              " '_getJavaStorageLevel',\n",
              " '_initialize_context',\n",
              " '_javaAccumulator',\n",
              " '_jsc',\n",
              " '_jvm',\n",
              " '_lock',\n",
              " '_next_accum_id',\n",
              " '_pickled_broadcast_vars',\n",
              " '_python_includes',\n",
              " '_repr_html_',\n",
              " '_serialize_to_jvm',\n",
              " '_temp_dir',\n",
              " '_unbatched_serializer',\n",
              " 'accumulator',\n",
              " 'addArchive',\n",
              " 'addFile',\n",
              " 'addJobTag',\n",
              " 'addPyFile',\n",
              " 'appName',\n",
              " 'applicationId',\n",
              " 'binaryFiles',\n",
              " 'binaryRecords',\n",
              " 'broadcast',\n",
              " 'cancelAllJobs',\n",
              " 'cancelJobGroup',\n",
              " 'cancelJobsWithTag',\n",
              " 'clearJobTags',\n",
              " 'defaultMinPartitions',\n",
              " 'defaultParallelism',\n",
              " 'dump_profiles',\n",
              " 'emptyRDD',\n",
              " 'environment',\n",
              " 'getCheckpointDir',\n",
              " 'getConf',\n",
              " 'getJobTags',\n",
              " 'getLocalProperty',\n",
              " 'getOrCreate',\n",
              " 'hadoopFile',\n",
              " 'hadoopRDD',\n",
              " 'listArchives',\n",
              " 'listFiles',\n",
              " 'master',\n",
              " 'newAPIHadoopFile',\n",
              " 'newAPIHadoopRDD',\n",
              " 'parallelize',\n",
              " 'pickleFile',\n",
              " 'profiler_collector',\n",
              " 'pythonExec',\n",
              " 'pythonVer',\n",
              " 'range',\n",
              " 'removeJobTag',\n",
              " 'resources',\n",
              " 'runJob',\n",
              " 'sequenceFile',\n",
              " 'serializer',\n",
              " 'setCheckpointDir',\n",
              " 'setInterruptOnCancel',\n",
              " 'setJobDescription',\n",
              " 'setJobGroup',\n",
              " 'setLocalProperty',\n",
              " 'setLogLevel',\n",
              " 'setSystemProperty',\n",
              " 'show_profiles',\n",
              " 'sparkHome',\n",
              " 'sparkUser',\n",
              " 'startTime',\n",
              " 'statusTracker',\n",
              " 'stop',\n",
              " 'textFile',\n",
              " 'uiWebUrl',\n",
              " 'union',\n",
              " 'version',\n",
              " 'wholeTextFiles']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dir(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK8Zbos1bJEk"
      },
      "outputs": [],
      "source": [
        "data = [1,2,3,4]\n",
        "dist = sc.parallelize(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg3rsjH9zpt0",
        "outputId": "650c5d82-c8c6-437f-fab7-41e7c096a183"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement spark-excel (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for spark-excel\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install spark-excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8dTGBz80NTI"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ExcelFileLoader\") \\\n",
        "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "0wPcPPNW0WPt",
        "outputId": "9e2a71cc-8ecc-40b4-ec6a-443fdfdaf329"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-43566055-a5de-4a79-b733-840944f3d968\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-43566055-a5de-4a79-b733-840944f3d968\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTmm9I1M1Gh2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pandas_df = pd.read_excel('/content/yahoo_data.xlsx')\n",
        "\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THBZU0dc1lTG"
      },
      "outputs": [],
      "source": [
        "df_rdd = sc.textFile('/content/yahoo_data.xlsx')\n",
        "df_rdd.take(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZec4Dkl3no3"
      },
      "outputs": [],
      "source": [
        "type(df_rdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucHgmXmo4EgF"
      },
      "outputs": [],
      "source": [
        "csv_rdd = df_rdd.map(lambda row: row.split(','))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYnsigIqBK1t"
      },
      "outputs": [],
      "source": [
        "# Install Java and Spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables for Java and Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "# Start a Spark session with the necessary package\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession with the spark-excel package\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read Excel File\") \\\n",
        "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.15.0\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Verify SparkSession\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkNd7PcaCLgd"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeXvyyq5CM54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read Excel File\") \\\n",
        "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHPZRBOUCNiA"
      },
      "outputs": [],
      "source": [
        "!wget -O sample.xlsx https://file-examples-com.github.io/uploads/2017/02/file_example_XLSX_10.xlsx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zREQutiT6TWI"
      },
      "outputs": [],
      "source": [
        "csv_rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i1vp9bN_MrD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pandas_df = pd.read_excel('/content/yahoo_data.xlsx')\n",
        "\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcWW5HZXEc9f"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay81gcyCI8QX"
      },
      "outputs": [],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcBmhxuxJH2m"
      },
      "outputs": [],
      "source": [
        "df.select('date','open').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgQ0q9Jsqx84"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKNQWDNXJbysGNxgtyuZe1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}